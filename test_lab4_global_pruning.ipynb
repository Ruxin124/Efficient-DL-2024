{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from models_cifar100.resnet import ResNet18\n",
    "# perform pruning on the model\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "# device check\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_test_accuracy 88.51\n"
     ]
    }
   ],
   "source": [
    "# We load the dictionary\n",
    "path1 = 'Session2_lab1_Resnet18_cifar10_binnary_tunning_2.pth'\n",
    "# loaded_cpt = torch.load(path1)\n",
    "loaded_cpt = torch.load(path1, map_location=device)\n",
    "# Fetch the hyperparam value\n",
    "# hparam_bestvalue = loaded_cpt['hyperparam']\n",
    "# print('hparam_bestvalue:', hparam_bestvalue)\n",
    "# accuracy\n",
    "print(\"best_test_accuracy\",loaded_cpt['final_test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct the model\n",
    "model_1 = ResNet18()\n",
    "\n",
    "# Load the model state dictionary with the loaded state dict\n",
    "model_1.load_state_dict(loaded_cpt['net'])\n",
    "\n",
    "# If you use this model for inference (= no further training), you need to set it into eval mode\n",
    "model_1.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform global pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = [\n",
    "    (module, 'weight') for module in model_1.modules() if isinstance(module, nn.Conv2d)\n",
    "]\n",
    "\n",
    "# Prune the model\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.5,\n",
    ")\n",
    "\n",
    "# save the pruned model\n",
    "torch.save({\n",
    "    'model_state_dict': model_1.state_dict(),\n",
    "    'pruning_config': {'method': prune.l1_unstructured, 'amount': 0.5}\n",
    "}, 'globale_pruned_0.5.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 50.46%\n",
      "Sparsity in Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 49.89%\n",
      "Sparsity in Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.55%\n",
      "Sparsity in Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.67%\n",
      "Sparsity in Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.46%\n",
      "Sparsity in Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 48.50%\n",
      "Sparsity in Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.04%\n",
      "Sparsity in Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False): 48.29%\n",
      "Sparsity in Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.10%\n",
      "Sparsity in Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.02%\n",
      "Sparsity in Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 49.44%\n",
      "Sparsity in Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 49.22%\n",
      "Sparsity in Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False): 49.00%\n",
      "Sparsity in Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.98%\n",
      "Sparsity in Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.66%\n",
      "Sparsity in Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 49.61%\n",
      "Sparsity in Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 49.80%\n",
      "Sparsity in Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False): 49.86%\n",
      "Sparsity in Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 50.01%\n",
      "Sparsity in Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 51.75%\n"
     ]
    }
   ],
   "source": [
    "# check the sparsity induced in every pruned parameter\n",
    "for layer, _ in parameters_to_prune:\n",
    "    print(\n",
    "        \"Sparsity in {}: {:.2f}%\".format(\n",
    "            layer, 100. * float(torch.sum(layer.weight == 0))\n",
    "            / float(layer.weight.nelement())\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# check Global sparsity\n",
    "all_weights = torch.cat([layer.weight.flatten() for layer, _ in parameters_to_prune])\n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(all_weights == 0))\n",
    "        / float(all_weights.nelement())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate the performance of pruned model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn as nn\n",
    "from models_cifar100.resnet import ResNet18\n",
    "\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "## Normalization adapted for CIFAR10\n",
    "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "# Transforms is a list of transformations applied on the 'raw' dataset before the data is fed to the network. \n",
    "# Here, Data augmentation (RandomCrop and Horizontal Flip) are applied to each batch, differently at each epoch, on the training set data only\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "### The data from CIFAR10 will be downloaded in the following folder\n",
    "rootdir = './data/cifar10'\n",
    "# Load the CIFAR-10 dataset\n",
    "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
    "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
    "# Create DataLoaders\n",
    "trainloader = DataLoader(c10train,batch_size=32,shuffle=True)\n",
    "testloader = DataLoader(c10test,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the Pruned Model Correctly\n",
    "- Reinitialize or recreate the model architecture.\n",
    "- Reapply the pruning to the model exactly as you did before saving.\n",
    "- Load the pruned state dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model with metadata\n",
    "checkpoint = torch.load('globale_pruned_0.5.pth')\n",
    "model_pruned = ResNet18()  \n",
    "for module in model_pruned.modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.5)\n",
    "model_pruned.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_pruned.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 16.65%\n"
     ]
    }
   ],
   "source": [
    "# test the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_pruned(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 11220132\n"
     ]
    }
   ],
   "source": [
    "# cout the number of parameters\n",
    "total_params = sum(p.numel() for p in model_pruned.parameters())\n",
    "print(f'Total parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Time\n",
    "The time it takes for the model to make predictions can also be a measure of its complexity, particularly from an operational standpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_inference_time(model, data_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure inference time\n",
    "inference_time = measure_inference_time(model_pruned, testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2606606483459473"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fin tunning the hyperparemeter after quantination "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "several strategies to recover the accuracy:\n",
    "\n",
    "Fine-Tuning: After applying binary quantization, you can fine-tune the quantized model on the training dataset. Even a few epochs of fine-tuning might help in recovering some of the lost accuracy.\n",
    "\n",
    "Hybrid Approaches: Instead of quantizing all layers to binary weights, you might consider a hybrid approach where only certain layers are quantized, or different quantization schemes (like ternary weights or lower-bit quantizations) are applied selectively.\n",
    "\n",
    "Quantization-Aware Training: Instead of applying quantization as a post-processing step, you can train the network with quantization in mind. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.04206761345267296\n",
      "Epoch 1: Loss 0.2428952157497406\n",
      "Epoch 2: Loss 0.004120262339711189\n",
      "Epoch 3: Loss 0.0010275698732584715\n",
      "Epoch 4: Loss 0.00659582344815135\n",
      "Epoch 5: Loss 0.0070841507986187935\n",
      "Epoch 6: Loss 0.08025392144918442\n",
      "Epoch 7: Loss 0.018089229241013527\n",
      "Epoch 8: Loss 0.045258451253175735\n",
      "Epoch 9: Loss 0.025238467380404472\n",
      "Epoch 10: Loss 0.053677886724472046\n",
      "Epoch 11: Loss 0.0006212434382177889\n",
      "Epoch 12: Loss 0.006726369261741638\n",
      "Epoch 13: Loss 0.004015504848212004\n",
      "Epoch 14: Loss 0.2850176692008972\n",
      "Epoch 15: Loss 0.001490758964791894\n",
      "Epoch 16: Loss 0.0038603274151682854\n",
      "Epoch 17: Loss 0.14339271187782288\n",
      "Epoch 18: Loss 0.13537806272506714\n",
      "Epoch 19: Loss 0.002384340390563011\n",
      "Epoch 20: Loss 0.007040188182145357\n",
      "Epoch 21: Loss 0.1538671851158142\n",
      "Epoch 22: Loss 0.01756676286458969\n",
      "Epoch 23: Loss 0.04192166030406952\n",
      "Epoch 24: Loss 0.004998200573027134\n",
      "Epoch 25: Loss 0.007983146235346794\n",
      "Epoch 26: Loss 0.017243223264813423\n",
      "Epoch 27: Loss 0.0010289873462170362\n",
      "Epoch 28: Loss 0.0001419628970324993\n",
      "Epoch 29: Loss 0.00786653347313404\n",
      "Epoch 30: Loss 0.3483163118362427\n",
      "Epoch 31: Loss 0.19264225661754608\n",
      "Epoch 32: Loss 0.2292182743549347\n",
      "Epoch 33: Loss 0.0013322115410119295\n",
      "Epoch 34: Loss 0.0010431939736008644\n",
      "Epoch 35: Loss 0.21492359042167664\n",
      "Epoch 36: Loss 0.009119957685470581\n",
      "Epoch 37: Loss 0.13131839036941528\n",
      "Epoch 38: Loss 0.00021940923761576414\n",
      "Epoch 39: Loss 0.0017128633335232735\n",
      "Epoch 40: Loss 0.00016000584582798183\n",
      "Epoch 41: Loss 0.0007528269779868424\n",
      "Epoch 42: Loss 0.005642781965434551\n",
      "Epoch 43: Loss 8.630527008790523e-05\n",
      "Epoch 44: Loss 0.0015075443079695106\n",
      "Epoch 45: Loss 0.0013549469877034426\n",
      "Epoch 46: Loss 0.06416793167591095\n",
      "Epoch 47: Loss 0.00010084905079565942\n",
      "Epoch 48: Loss 0.0045033348724246025\n",
      "Epoch 49: Loss 0.007161293178796768\n",
      "Finished fine-tuning\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# fine-tune a binary quantized model in PyTorch\n",
    "model_2.train()\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr=0.001)  # Using a lower learning rate for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_fine_tune_epochs = 50\n",
    "# Fine-tuning loop\n",
    "\n",
    "# train loop\n",
    "for epoch in range(num_fine_tune_epochs):\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model_2(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch}: Loss {loss.item()}')\n",
    "\n",
    "print('Finished fine-tuning')\n",
    "\n",
    "# save the model \n",
    "torch.save({\n",
    "    'net': model_2.state_dict(),\n",
    "    'final_test_accuracy': test_accuracy,\n",
    "    'hyperparam': hparam_bestvalue,\n",
    "}, 'Session2_lab1_Resnet18_cifar10_binnary_tunning_2.pth')\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtestloader\u001b[49m:\n\u001b[0;32m     16\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     17\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'testloader' is not defined"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "path3 = 'Session2_lab1_Resnet18_cifar10_binnary_tunning_2.pth'\n",
    "loaded_cpt = torch.load(path3)\n",
    "\n",
    "# construct the model\n",
    "model_3 = ResNet18()\n",
    "# Load the model state dictionary with the loaded state dict\n",
    "model_3.load_state_dict(loaded_cpt['net'])\n",
    "model_3.eval().to(device)\n",
    "\n",
    "# test the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_3(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized Model after tunning Inference Time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "inference_time_quantized = measure_inference_time(model_3, testloader, device )\n",
    "print(f\"Quantized Model after tunning Inference Time: {inference_time_quantized:.2f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tunned quantized model parameters: 11220132\n"
     ]
    }
   ],
   "source": [
    "tuned_quantized_params = count_parameters(model_3)\n",
    "print(f'Tunned quantized model parameters: {tuned_quantized_params}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 88.51%\n"
     ]
    }
   ],
   "source": [
    "# save the model \n",
    "torch.save({\n",
    "    'net': model_2.state_dict(),\n",
    "    'final_test_accuracy': test_accuracy,\n",
    "    'hyperparam': hparam_bestvalue,\n",
    "}, 'Session2_lab1_Resnet18_cifar10_binnary_tunning_1.pth')\n",
    "\n",
    "# test the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_2(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 42.88 MB\n",
      "Quantized model size: 42.88 MB\n",
      "Tuned quantized model (10 epochs) size: 42.88 MB\n",
      "Tuned quantized model (50 epochs) size: 42.88 MB\n"
     ]
    }
   ],
   "source": [
    "path4 = \"Session2_lab1_Resnet18_cifar10_binnary_tunning_1.pth\"\n",
    "import os\n",
    "\n",
    "original = path1\n",
    "quantized = path2\n",
    "tuned_quantized_10= path4\n",
    "tuned_quantized_50 = path3\n",
    "\n",
    "# Function to convert bytes to megabytes\n",
    "def bytes_to_mb(size_in_bytes):\n",
    "    return size_in_bytes / 1024 / 1024\n",
    "\n",
    "# Measure the file sizes\n",
    "original_size = os.path.getsize(original)\n",
    "quantized_size = os.path.getsize(quantized)\n",
    "tuned_quantized_10_size = os.path.getsize(tuned_quantized_10)\n",
    "tuned_quantized_50_size = os.path.getsize(tuned_quantized_50)\n",
    "\n",
    "print(f\"Original model size: {bytes_to_mb(original_size):.2f} MB\")\n",
    "print(f\"Quantized model size: {bytes_to_mb(quantized_size):.2f} MB\")\n",
    "print(f\"Tuned quantized model (10 epochs) size: {bytes_to_mb(tuned_quantized_10_size):.2f} MB\")\n",
    "print(f\"Tuned quantized model (50 epochs) size: {bytes_to_mb(tuned_quantized_50_size):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
