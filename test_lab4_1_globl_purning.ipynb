{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from models_cifar100.resnet import ResNet18\n",
    "# perform pruning on the model\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU.\n"
     ]
    }
   ],
   "source": [
    "# device check\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_test_accuracy 88.51\n"
     ]
    }
   ],
   "source": [
    "# We load the dictionary\n",
    "path1 = 'Session2_lab1_Resnet18_cifar10_binnary_tunning_2.pth'\n",
    "# loaded_cpt = torch.load(path1)\n",
    "loaded_cpt = torch.load(path1, map_location=device)\n",
    "# Fetch the hyperparam value\n",
    "# hparam_bestvalue = loaded_cpt['hyperparam']\n",
    "# print('hparam_bestvalue:', hparam_bestvalue)\n",
    "# accuracy\n",
    "print(\"best_test_accuracy\",loaded_cpt['final_test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct the model\n",
    "model_1 = ResNet18()\n",
    "\n",
    "# Load the model state dictionary with the loaded state dict\n",
    "model_1.load_state_dict(loaded_cpt['net'])\n",
    "\n",
    "# If you use this model for inference (= no further training), you need to set it into eval mode\n",
    "model_1.eval().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perform global pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_to_prune = [\n",
    "    (module, 'weight') for module in model_1.modules() if isinstance(module, nn.Conv2d)\n",
    "]\n",
    "\n",
    "# Prune the model\n",
    "prune.global_unstructured(\n",
    "    parameters_to_prune,\n",
    "    pruning_method=prune.L1Unstructured,\n",
    "    amount=0.5,\n",
    ")\n",
    "\n",
    "# save the pruned model\n",
    "torch.save({\n",
    "    'model_state_dict': model_1.state_dict(),\n",
    "    'pruning_config': {'method': prune.l1_unstructured, 'amount': 0.5}\n",
    "}, 'globale_pruned_0.5.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sparsity in Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 50.46%\n",
      "Sparsity in Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 49.89%\n",
      "Sparsity in Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.55%\n",
      "Sparsity in Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.67%\n",
      "Sparsity in Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.46%\n",
      "Sparsity in Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 48.50%\n",
      "Sparsity in Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.04%\n",
      "Sparsity in Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False): 48.29%\n",
      "Sparsity in Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.10%\n",
      "Sparsity in Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.02%\n",
      "Sparsity in Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 49.44%\n",
      "Sparsity in Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 49.22%\n",
      "Sparsity in Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False): 49.00%\n",
      "Sparsity in Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.98%\n",
      "Sparsity in Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 48.66%\n",
      "Sparsity in Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False): 49.61%\n",
      "Sparsity in Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 49.80%\n",
      "Sparsity in Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False): 49.86%\n",
      "Sparsity in Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 50.01%\n",
      "Sparsity in Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False): 51.75%\n"
     ]
    }
   ],
   "source": [
    "# check the sparsity induced in every pruned parameter\n",
    "for layer, _ in parameters_to_prune:\n",
    "    print(\n",
    "        \"Sparsity in {}: {:.2f}%\".format(\n",
    "            layer, 100. * float(torch.sum(layer.weight == 0))\n",
    "            / float(layer.weight.nelement())\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global sparsity: 50.00%\n"
     ]
    }
   ],
   "source": [
    "# check Global sparsity\n",
    "all_weights = torch.cat([layer.weight.flatten() for layer, _ in parameters_to_prune])\n",
    "print(\n",
    "    \"Global sparsity: {:.2f}%\".format(\n",
    "        100. * float(torch.sum(all_weights == 0))\n",
    "        / float(all_weights.nelement())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate the performance of pruned model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import torch.nn as nn\n",
    "from models_cifar100.resnet import ResNet18\n",
    "\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "## Normalization adapted for CIFAR10\n",
    "normalize_scratch = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "# Transforms is a list of transformations applied on the 'raw' dataset before the data is fed to the network. \n",
    "# Here, Data augmentation (RandomCrop and Horizontal Flip) are applied to each batch, differently at each epoch, on the training set data only\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_scratch,\n",
    "])\n",
    "### The data from CIFAR10 will be downloaded in the following folder\n",
    "rootdir = './data/cifar10'\n",
    "# Load the CIFAR-10 dataset\n",
    "c10train = CIFAR10(rootdir,train=True,download=True,transform=transform_train)\n",
    "c10test = CIFAR10(rootdir,train=False,download=True,transform=transform_test)\n",
    "# Create DataLoaders\n",
    "trainloader = DataLoader(c10train,batch_size=32,shuffle=True)\n",
    "testloader = DataLoader(c10test,batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reload the Pruned Model Correctly\n",
    "- Reinitialize or recreate the model architecture.\n",
    "- Reapply the pruning to the model exactly as you did before saving.\n",
    "- Load the pruned state dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model with metadata\n",
    "checkpoint = torch.load('globale_pruned_0.2.pth')\n",
    "model_pruned = ResNet18()  \n",
    "for module in model_pruned.modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        prune.l1_unstructured(module, name='weight', amount=0.2)\n",
    "model_pruned.load_state_dict(checkpoint['model_state_dict'])\n",
    "model_pruned.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 79.02%\n"
     ]
    }
   ],
   "source": [
    "# test the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_pruned(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 11220132\n"
     ]
    }
   ],
   "source": [
    "# cout the number of parameters\n",
    "total_params = sum(p.numel() for p in model_pruned.parameters())\n",
    "print(f'Total parameters: {total_params}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference Time\n",
    "The time it takes for the model to make predictions can also be a measure of its complexity, particularly from an operational standpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_inference_time(model, data_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for data, _ in data_loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.609161853790283"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure inference time\n",
    "inference_time = measure_inference_time(model_pruned, testloader, device)\n",
    "inference_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrain after globale pruning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 0.37452584505081177\n",
      "Epoch 1: Loss 0.23368561267852783\n",
      "Epoch 2: Loss 0.25590336322784424\n",
      "Epoch 3: Loss 0.10860403627157211\n",
      "Epoch 4: Loss 0.0019217527005821466\n",
      "Epoch 5: Loss 0.04290783405303955\n",
      "Epoch 6: Loss 0.0012900868896394968\n",
      "Epoch 7: Loss 0.0641978457570076\n",
      "Epoch 8: Loss 0.0013867481611669064\n",
      "Epoch 9: Loss 0.10195066779851913\n",
      "Epoch 10: Loss 0.00020884550758637488\n",
      "Epoch 11: Loss 0.0038286964409053326\n",
      "Epoch 12: Loss 0.024403071030974388\n",
      "Epoch 13: Loss 0.09032944589853287\n",
      "Epoch 14: Loss 0.09677796065807343\n",
      "Epoch 15: Loss 0.04736460745334625\n",
      "Epoch 16: Loss 0.41000503301620483\n",
      "Epoch 17: Loss 0.00012129008246120065\n",
      "Epoch 18: Loss 2.9338709282455966e-05\n",
      "Epoch 19: Loss 0.010061814449727535\n",
      "Epoch 20: Loss 0.0011490131728351116\n",
      "Epoch 21: Loss 0.047677699476480484\n",
      "Epoch 22: Loss 0.0010111347073689103\n",
      "Epoch 23: Loss 0.0013076342875137925\n",
      "Epoch 24: Loss 0.0001607104204595089\n",
      "Epoch 25: Loss 0.0034513315185904503\n",
      "Epoch 26: Loss 0.005817738361656666\n",
      "Epoch 27: Loss 0.0002356541808694601\n",
      "Epoch 28: Loss 0.0035463497042655945\n",
      "Epoch 29: Loss 0.0023146693129092455\n",
      "Epoch 30: Loss 0.00013979092182125896\n",
      "Epoch 31: Loss 0.00040053908014670014\n",
      "Epoch 32: Loss 9.425953612662852e-05\n",
      "Epoch 33: Loss 0.020164230838418007\n",
      "Epoch 34: Loss 0.00013218613457866013\n",
      "Epoch 35: Loss 0.0990651473402977\n",
      "Epoch 36: Loss 0.00027670839335769415\n",
      "Epoch 37: Loss 0.00042555772233754396\n",
      "Epoch 38: Loss 0.0026824527885764837\n",
      "Epoch 39: Loss 0.018048761412501335\n",
      "Epoch 40: Loss 3.141412162221968e-05\n",
      "Epoch 41: Loss 0.00019357340352144092\n",
      "Epoch 42: Loss 0.00040893658297136426\n",
      "Epoch 43: Loss 0.0657690018415451\n",
      "Epoch 44: Loss 0.005179709289222956\n",
      "Epoch 45: Loss 0.0014798076590523124\n",
      "Epoch 46: Loss 0.0036385993007570505\n",
      "Epoch 47: Loss 0.00010669991024769843\n",
      "Epoch 48: Loss 0.011696401983499527\n",
      "Epoch 49: Loss 0.009059187956154346\n",
      "Finished fine-tuning\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "# fine-tune a binary quantized model in PyTorch\n",
    "model_pruned.train()\n",
    "optimizer = torch.optim.Adam(model_pruned.parameters(), lr=0.001)  # Using a lower learning rate for fine-tuning\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_fine_tune_epochs = 50\n",
    "# Fine-tuning loop\n",
    "\n",
    "# train loop\n",
    "for epoch in range(num_fine_tune_epochs):\n",
    "    for data, target in trainloader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model_pruned(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch}: Loss {loss.item()}')\n",
    "\n",
    "print('Finished fine-tuning')\n",
    "\n",
    "# save the model \n",
    "torch.save({\n",
    "    'net': model_pruned.state_dict(),\n",
    "    'final_test_accuracy': test_accuracy,\n",
    "}, 'globale_pruned_0.2_retrained_50epochs.pth')\n",
    "print('Model saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=512, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model with metadata\n",
    "path = 'globale_pruned_0.2_retrained_50epochs.pth'\n",
    "checkpoint = torch.load(path)\n",
    "\n",
    "model_pruned_retrain = ResNet18()  \n",
    "# make sure to prune the model before loading the state dict\n",
    "for module in model_pruned_retrain.modules():\n",
    "     if isinstance(module, nn.Conv2d):\n",
    "         prune.l1_unstructured(module, name='weight', amount=0.2)\n",
    "model_pruned_retrain.load_state_dict(checkpoint['net'])\n",
    "model_pruned_retrain.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 90.79%\n"
     ]
    }
   ],
   "source": [
    "# test the model on the test set\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model_pruned_retrain(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 42.88 MB\n",
      "Quantized model size: 42.88 MB\n",
      "Tuned quantized model (10 epochs) size: 42.88 MB\n",
      "Tuned quantized model (50 epochs) size: 42.88 MB\n"
     ]
    }
   ],
   "source": [
    "path4 = \"Session2_lab1_Resnet18_cifar10_binnary_tunning_1.pth\"\n",
    "import os\n",
    "\n",
    "original = path1\n",
    "quantized = path2\n",
    "tuned_quantized_10= path4\n",
    "tuned_quantized_50 = path3\n",
    "\n",
    "# Function to convert bytes to megabytes\n",
    "def bytes_to_mb(size_in_bytes):\n",
    "    return size_in_bytes / 1024 / 1024\n",
    "\n",
    "# Measure the file sizes\n",
    "original_size = os.path.getsize(original)\n",
    "quantized_size = os.path.getsize(quantized)\n",
    "tuned_quantized_10_size = os.path.getsize(tuned_quantized_10)\n",
    "tuned_quantized_50_size = os.path.getsize(tuned_quantized_50)\n",
    "\n",
    "print(f\"Original model size: {bytes_to_mb(original_size):.2f} MB\")\n",
    "print(f\"Quantized model size: {bytes_to_mb(quantized_size):.2f} MB\")\n",
    "print(f\"Tuned quantized model (10 epochs) size: {bytes_to_mb(tuned_quantized_10_size):.2f} MB\")\n",
    "print(f\"Tuned quantized model (50 epochs) size: {bytes_to_mb(tuned_quantized_50_size):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
